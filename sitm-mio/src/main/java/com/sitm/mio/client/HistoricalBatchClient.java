package com.sitm.mio.client;

import java.io.IOException;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

import com.sitm.mio.graphs.GraphVisualizer;
import com.sitm.mio.util.ConfigManager;
import com.sitm.mio.util.StreamingDatagramReader;

import Ice.Communicator;
import Ice.ObjectPrx;
import Ice.Util;
import SITM.MIO.BusDatagram;
import SITM.MIO.MasterPrx;
import SITM.MIO.MasterPrxHelper;
import SITM.MIO.VelocityResult;

/**
 * Cliente para procesamiento de datos HIST√ìRICOS masivos (36GB+)
 * Usa batch processing con streaming para no saturar memoria
 */
public class HistoricalBatchClient {
    
    private Communicator communicator;
    private MasterPrx master;
    
    public void initialize(String[] args) {
        communicator = Util.initialize(args);
        
        ConfigManager config = ConfigManager.getInstance();
        String masterHost = config.getString("master.host", "localhost");
        int masterPort = config.getInt("master.port", 10000);
        String masterEndpoint = "tcp -h " + masterHost + " -p " + masterPort;
        
        ObjectPrx base = communicator.stringToProxy("Master:" + masterEndpoint);
        master = MasterPrxHelper.checkedCast(base);
        
        if (master == null) {
            throw new Error("Invalid master proxy");
        }
        
        System.out.println("‚úì Conectado al Master en: " + masterEndpoint);
    }
    
    /**
     * Procesa archivo hist√≥rico grande por lotes - OPTIMIZADO PARA 36GB
     */
    public void processHistoricalFile(String filePath, String dataPath) {
        System.out.println("\n" + "=".repeat(80));
        System.out.println("PROCESAMIENTO HIST√ìRICO - DATOS MASIVOS (36GB+)");
        System.out.println("=".repeat(80));
        System.out.println("Archivo: " + filePath);
        
        try {
            // 1. An√°lisis del archivo
            long fileSize = StreamingDatagramReader.getFileSize(filePath);
            long estimatedLines = StreamingDatagramReader.countLines(filePath);
            
            System.out.println("\nüìä AN√ÅLISIS DEL ARCHIVO:");
            System.out.printf("  ‚Ä¢ Tama√±o: %.2f GB%n", fileSize / (1024.0 * 1024.0 * 1024.0));
            System.out.printf("  ‚Ä¢ L√≠neas estimadas: %,d%n", estimatedLines);
            
            // 2. Configurar tama√±o de lote seg√∫n configuraci√≥n
            ConfigManager config = ConfigManager.getInstance();
            int batchSize = config.getInt("processing.batch.size", 100000); // Aumentado para 36GB
            System.out.println("\nüñ• Estado del cluster: " + master.getSystemStatus());
            System.out.printf("  ‚Ä¢ Tama√±o de lote: %,d datagramas%n", batchSize);
            
            // 3. Procesamiento por lotes
            long globalStartTime = System.currentTimeMillis();
            List<VelocityResult> allResults = new ArrayList<>();
            int batchNumber = 0;
            long totalProcessed = 0;
            
            try (StreamingDatagramReader reader = new StreamingDatagramReader(filePath, batchSize)) {
                
                BusDatagram[] batch;
                while ((batch = reader.readNextBatch()) != null) {
                    batchNumber++;
                    
                    System.out.println("\n" + "-".repeat(80));
                    System.out.printf("LOTE #%d - %,d datagramas%n", batchNumber, batch.length);
                    
                    long batchStartTime = System.currentTimeMillis();
                    
                    // Enviar lote al cluster distribuido - CORREGIDO: solo 3 par√°metros
                    VelocityResult[] batchResults = master.processHistoricalData(batch, null, null);
                    
                    long batchEndTime = System.currentTimeMillis();
                    long batchTime = batchEndTime - batchStartTime;
                    
                    // Agregar resultados
                    allResults.addAll(Arrays.asList(batchResults));
                    totalProcessed += batch.length;
                    
                    // M√©tricas del lote
                    double throughput = (batch.length / (double) batchTime) * 1000;
                    System.out.printf("  ‚è± Tiempo: %,d ms%n", batchTime);
                    System.out.printf("  ‚ö° Throughput: %.2f datagramas/seg%n", throughput);
                    System.out.printf("  üìà Progreso: %,d / %,d (%.1f%%)%n", 
                        totalProcessed, estimatedLines, 
                        (totalProcessed * 100.0 / estimatedLines));
                    
                    // Pausa para evitar saturar el cluster
                    if (batchNumber % 10 == 0) {
                        Thread.sleep(500);
                    }
                }
            }
            
            long globalEndTime = System.currentTimeMillis();
            long totalTime = globalEndTime - globalStartTime;
            
            // 4. Resultados consolidados
            printConsolidatedResults(allResults, totalProcessed, totalTime, batchNumber);
            
            // 5. Visualizaci√≥n (solo si hay resultados)
            if (allResults.size() > 0) {
                generateVisualization(dataPath, allResults);
            }
            
        } catch (IOException e) {
            System.err.println("‚ùå Error de lectura: " + e.getMessage());
            e.printStackTrace();
        } catch (Exception e) {
            System.err.println("‚ùå Error de procesamiento: " + e.getMessage());
            e.printStackTrace();
        }
    }
    
    private void printConsolidatedResults(List<VelocityResult> results, 
                                         long totalProcessed, 
                                         long totalTime,
                                         int totalBatches) {
        System.out.println("\n" + "=".repeat(80));
        System.out.println("RESULTADOS CONSOLIDADOS - PROCESAMIENTO HIST√ìRICO");
        System.out.println("=".repeat(80));
        
        System.out.printf("üì¶ Datagramas procesados: %,d%n", totalProcessed);
        System.out.printf("‚è± Tiempo total: %,d ms (%.2f minutos)%n", 
            totalTime, totalTime / 60000.0);
        System.out.printf("üî¢ Lotes procesados: %d%n", totalBatches);
        
        // CORREGIDO: Evitar divisi√≥n por cero
        double globalThroughput = totalTime > 0 ? (totalProcessed / (double) totalTime) * 1000 : 0;
        System.out.printf("‚ö° Throughput global: %.2f datagramas/seg%n", globalThroughput);
        
        // Consolidar resultados por arco
        Map<String, ArcStats> statsByArc = new HashMap<>();
        
        for (VelocityResult result : results) {
            // FILTRO CR√çTICO: Solo considerar resultados v√°lidos
            if (result.sampleCount > 0 && result.averageVelocity > 0) {
                ArcStats stats = statsByArc.computeIfAbsent(
                    result.arcId, 
                    k -> new ArcStats()
                );
                
                stats.totalVelocity += result.averageVelocity * result.sampleCount;
                stats.totalSamples += result.sampleCount;
            }
        }
        
        System.out.println("\nüìä ESTAD√çSTICAS GLOBALES:");
        System.out.printf("  ‚Ä¢ Arcos con datos: %,d%n", statsByArc.size());
        System.out.printf("  ‚Ä¢ Muestras totales: %,d%n", 
            statsByArc.values().stream().mapToLong(s -> s.totalSamples).sum());
        
        // C√°lculo seguro del promedio global
        double globalAvg = statsByArc.values().stream()
            .filter(s -> s.totalSamples > 0)
            .mapToDouble(s -> s.totalVelocity / s.totalSamples)
            .average()
            .orElse(0.0);
        
        System.out.printf("  ‚Ä¢ Velocidad promedio global: %.2f m/s (%.1f km/h)%n", 
            globalAvg, globalAvg * 3.6);
        
        // Top 10 arcos m√°s r√°pidos (solo con muestras suficientes)
        System.out.println("\nüèÜ TOP 10 ARCOS M√ÅS R√ÅPIDOS:");
        statsByArc.entrySet().stream()
            .filter(e -> e.getValue().totalSamples >= 50) // M√≠nimo 50 muestras para estad√≠stica confiable
            .sorted((a, b) -> Double.compare(
                b.getValue().getAverageVelocity(),
                a.getValue().getAverageVelocity()
            ))
            .limit(10)
            .forEach(e -> {
                ArcStats stats = e.getValue();
                double avg = stats.getAverageVelocity();
                System.out.printf("  %s: %.2f km/h (%,d muestras)%n", 
                    e.getKey(), avg * 3.6, stats.totalSamples);
            });
            
        // Tambi√©n mostrar arcos m√°s lentos para an√°lisis
        System.out.println("\nüê¢ TOP 5 ARCOS M√ÅS LENTOS:");
        statsByArc.entrySet().stream()
            .filter(e -> e.getValue().totalSamples >= 50)
            .sorted((a, b) -> Double.compare(
                a.getValue().getAverageVelocity(),
                b.getValue().getAverageVelocity()
            ))
            .limit(5)
            .forEach(e -> {
                ArcStats stats = e.getValue();
                double avg = stats.getAverageVelocity();
                System.out.printf("  %s: %.2f km/h (%,d muestras)%n", 
                    e.getKey(), avg * 3.6, stats.totalSamples);
            });
    }
    
    private void generateVisualization(String dataPath, List<VelocityResult> results) {
        try {
            System.out.println("\nüìä Generando visualizaci√≥n del grafo...");
            
            GraphVisualizer visualizer = new GraphVisualizer();
            visualizer.loadData(dataPath);
            visualizer.loadVelocities(results.toArray(new VelocityResult[0]));
            
            javax.swing.JFrame frame = new javax.swing.JFrame(
                "SITM-MIO - Velocidades Hist√≥ricas (36GB procesados)"
            );
            frame.setDefaultCloseOperation(javax.swing.JFrame.DISPOSE_ON_CLOSE);
            frame.add(visualizer);
            frame.pack();
            frame.setLocationRelativeTo(null);
            frame.setVisible(true);
            
            String timestamp = String.valueOf(System.currentTimeMillis());
            String outputFile = "grafo_historico_" + timestamp + ".jpg";
            visualizer.exportToJPG(outputFile);
            
            System.out.println("‚úì Visualizaci√≥n guardada: " + outputFile);
            
        } catch (Exception e) {
            System.err.println("‚ö† Error en visualizaci√≥n: " + e.getMessage());
        }
    }
    
    public void shutdown() {
        if (communicator != null) {
            communicator.destroy();
        }
    }
    
    static class ArcStats {
        double totalVelocity = 0;
        long totalSamples = 0;
        
        double getAverageVelocity() {
            return totalSamples > 0 ? totalVelocity / totalSamples : 0.0;
        }
    }
    
    public static void main(String[] args) {
        if (args.length < 2) {
            System.out.println("Uso: HistoricalBatchClient <archivo_csv_historico> <directorio_datos>");
            System.out.println("Ejemplo: HistoricalBatchClient ./data/datagrams_36gb.csv ./data");
            System.out.println("NOTA: Para archivos de 36GB+, usar batch size de 100,000 en config");
            return;
        }
        
        String historicalFile = args[0];
        String dataPath = args[1];
        
        HistoricalBatchClient client = new HistoricalBatchClient();
        
        try {
            client.initialize(args);
            client.processHistoricalFile(historicalFile, dataPath);
            
            System.out.println("\n‚úì Procesamiento hist√≥rico de 36GB completado exitosamente.");
            System.out.println("Presiona ENTER para salir...");
            System.in.read();
            
        } catch (Exception e) {
            System.err.println("‚ùå Error: " + e.getMessage());
            e.printStackTrace();
        } finally {
            client.shutdown();
        }
    }
}